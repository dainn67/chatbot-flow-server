# ğŸš€ HÆ°á»›ng dáº«n Streaming API

## Tá»•ng quan

Server nÃ y há»— trá»£ streaming responses tá»« cáº£ **Google Gemini** vÃ  **OpenAI GPT** thÃ´ng qua Server-Sent Events (SSE).

## CÃ¡c tÃ­nh nÄƒng Ä‘Ã£ thÃªm

### 1. **Streaming Functions**

#### OpenAI GPT Streaming
```python
from app.services.openai_service import stream_gpt

async for chunk in stream_gpt("Your prompt here"):
    print(chunk, end='', flush=True)
```

#### Gemini Streaming
```python
from app.services.gemini_service import stream_gemini

async for chunk in stream_gemini("Your prompt here"):
    print(chunk, end='', flush=True)
```

#### Unified LLM Streaming
```python
from app.services.llm_service import stream_llm

# Sá»­ dá»¥ng vá»›i Gemini
async for chunk in stream_llm("gemini", "Your prompt"):
    print(chunk, end='', flush=True)

# Sá»­ dá»¥ng vá»›i OpenAI
async for chunk in stream_llm("openai", "Your prompt"):
    print(chunk, end='', flush=True)
```

### 2. **Stream Test Flow**

File `app/flows/stream_test_flow.py` cung cáº¥p workflow vÃ­ dá»¥:

```python
from app.flows.stream_test_flow import run_stream_test_workflow

async for chunk in run_stream_test_workflow("Hello!", provider="gemini"):
    print(chunk, end='', flush=True)
```

### 3. **API Endpoints**

#### POST /api/stream/chat

Stream chat response vá»›i request body:

```bash
curl -X POST http://localhost:8000/api/stream/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user_input": "Xin chÃ o, báº¡n cÃ³ khá»e khÃ´ng?",
    "provider": "gemini"
  }'
```

#### GET /api/stream/chat-get

Stream chat response vá»›i query parameters:

```bash
curl "http://localhost:8000/api/stream/chat-get?user_input=Hello&provider=openai"
```

### 4. **Response Format**

Táº¥t cáº£ streaming endpoints tráº£ vá» SSE format:

```
data: {"chunk": "text chunk here", "done": false}
data: {"chunk": "more text", "done": false}
data: {"chunk": "", "done": true}
```

Náº¿u cÃ³ lá»—i:
```
data: {"error": "error message", "done": true}
```

## CÃ¡ch sá»­ dá»¥ng

### 1. Khá»Ÿi Ä‘á»™ng server

```bash
# Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Cháº¡y server
python -m app.main
```

Server sáº½ cháº¡y táº¡i: http://localhost:8000

### 2. Kiá»ƒm tra API Documentation

Truy cáº­p: http://localhost:8000/docs

### 3. Test vá»›i HTML Client

Má»Ÿ file `test_stream.html` trong trÃ¬nh duyá»‡t Ä‘á»ƒ test streaming trá»±c tiáº¿p:

1. Chá»n provider (Gemini hoáº·c OpenAI)
2. Nháº­p cÃ¢u há»i
3. Nháº¥n "Gá»­i vÃ  Stream Response"
4. Xem response Ä‘Æ°á»£c stream theo thá»i gian thá»±c

### 4. Test vá»›i Python Client

```python
import requests
import json

url = "http://localhost:8000/api/stream/chat-get"
params = {
    "user_input": "Giáº£i thÃ­ch vá» AI",
    "provider": "gemini"
}

response = requests.get(url, params=params, stream=True)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('data: '):
            data = json.loads(line[6:])
            if not data.get('done'):
                print(data['chunk'], end='', flush=True)
            else:
                print("\nâœ… Done!")
                break
```

### 5. Test vá»›i JavaScript (Browser)

```javascript
const eventSource = new EventSource(
    'http://localhost:8000/api/stream/chat-get?user_input=Hello&provider=gemini'
);

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.done) {
        console.log('Stream completed!');
        eventSource.close();
    } else {
        console.log(data.chunk);
    }
};

eventSource.onerror = (error) => {
    console.error('Error:', error);
    eventSource.close();
};
```

## Cáº¥u hÃ¬nh

Äáº£m báº£o file `.env` cÃ³ cÃ¡c API keys cáº§n thiáº¿t:

```env
OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
```

## LÆ°u Ã½

1. **Gemini Streaming**: Sá»­ dá»¥ng thread pool Ä‘á»ƒ xá»­ lÃ½ sync API cá»§a Gemini
2. **OpenAI Streaming**: Sá»­ dá»¥ng native async streaming cá»§a OpenAI SDK
3. **CORS**: ÄÃ£ cáº¥u hÃ¬nh cho phÃ©p táº¥t cáº£ origins (nÃªn giá»›i háº¡n trong production)
4. **Connection Keep-Alive**: Headers Ä‘Ã£ Ä‘Æ°á»£c set Ä‘á»ƒ maintain SSE connection

## Troubleshooting

### Server khÃ´ng stream response
- Kiá»ƒm tra API keys Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng chÆ°a
- Xem logs trong console
- Test health endpoint: http://localhost:8000/health

### Client khÃ´ng nháº­n Ä‘Æ°á»£c chunks
- Äáº£m báº£o sá»­ dá»¥ng `stream=True` khi request vá»›i Python
- Vá»›i JavaScript, dÃ¹ng `EventSource` API
- Kiá»ƒm tra CORS settings náº¿u gá»i tá»« domain khÃ¡c

### Response bá»‹ delay
- ÄÃ¢y lÃ  behavior bÃ¬nh thÆ°á»ng cá»§a streaming
- Gemini cÃ³ thá»ƒ cÃ³ Ä‘á»™ trá»… cao hÆ¡n OpenAI
- Network latency cÅ©ng áº£nh hÆ°á»Ÿng Ä‘áº¿n streaming speed

## API Examples

### Demo SSE Endpoint
```bash
curl http://localhost:8000/api/stream/sse
```

### Health Check
```bash
curl http://localhost:8000/health
```

### Non-streaming Chat (Original)
```bash
curl -X POST http://localhost:8000/api/flows/ask \
  -H "Content-Type: application/json" \
  -d '{"user_input": "Hello"}'
```

## Káº¿t luáº­n

Báº¡n Ä‘Ã£ cÃ³ Ä‘áº§y Ä‘á»§ cÃ¡c cÃ´ng cá»¥ Ä‘á»ƒ:
- âœ… Stream responses tá»« Gemini
- âœ… Stream responses tá»« OpenAI GPT
- âœ… Táº¡o custom streaming flows
- âœ… Test streaming vá»›i HTML client
- âœ… Integrate streaming vÃ o á»©ng dá»¥ng cá»§a báº¡n

Happy coding! ğŸ‰

